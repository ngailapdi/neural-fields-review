[{"# of input views (e.g. 18 for 18-camera system)":"","Abstract":"A hallmark of the deep learning era for computer vision is the successful use of large-scale labeled datasets to train feature representations for tasks ranging from object recognition and semantic segmentation to optical flow estimation and novel view synthesis of 3D scenes. In this work, we aim to learn dense discriminative object representations for low-shot category recognition without requiring any category labels. To this end, we propose Deep Object Patch Encodings (DOPE), which can be trained from multiple views of object instances without any category or semantic object part labels. To train DOPE, we assume access to sparse depths, foreground masks and known cameras, to obtain pixel-level correspondences between views of an object, and use this to formulate a self-supervised learning task to learn discriminative object patches. We find that DOPE can directly be used for low-shot classification of novel categories using local-part matching, and is competitive with and outperforms supervised and self-supervised learning baselines. ","Authors (format: First Last, First Middle Last, ...)":"Stefan Stojanov, Anh Thai, Zixuan Huang, James M. Rehg","Bibtex (e.g. @inproceedings...)":"@inproceedings{stojanovlearning,   title={Learning Dense Object Descriptors from Multiple Views for Low-shot Category Generalization},   author={Stojanov, Stefan and Thai, Ngoc Anh and Huang, Zixuan and Rehg, James Matthew},   booktitle={Advances in Neural Information Processing Systems} }","Bibtex Name":"stojanovlearning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/rehg-lab/dope_selfsup","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/28/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"sstojanov@gatech.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Self-supervised Learning, Low-shot Learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DOPE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2211.15059.pdf","Project webpage link":"https://sstojanov.github.io/projects/dope_selfsup.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=qaArkLiiymk","Timestamp":"10/26/2022 15:20:55","Title":"Learning Dense Object Descriptors from Multiple Views for Low-shot Category Generalization","Training time (hr)":"","UID":"395","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2022","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"}]
